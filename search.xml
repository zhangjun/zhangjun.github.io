<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>TNN</title>
    <url>/content/2021/03/06/TNN/</url>
    <content><![CDATA[<p>本文主要介绍Tencent TNN编译使用。</p>
<h1 id="下载编译"><a href="#下载编译" class="headerlink" title="下载编译"></a>下载编译</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone github.com/Tencent/TNN</span><br></pre></td></tr></table></figure>
<p>其他需要cmake、opencv，单独安装</p>
<h2 id="linux-x86"><a href="#linux-x86" class="headerlink" title="linux x86"></a>linux x86</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake .. -DTNN_X86_ENABLE&#x3D;ON</span><br></pre></td></tr></table></figure>


<h1 id="模型部署示例"><a href="#模型部署示例" class="headerlink" title="模型部署示例"></a>模型部署示例</h1><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> main();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>cmake</title>
    <url>/content/2021/03/01/article/cmake/</url>
    <content><![CDATA[<h1 id="cmake"><a href="#cmake" class="headerlink" title="cmake"></a>cmake</h1><h2 id="cmake-usage"><a href="#cmake-usage" class="headerlink" title="cmake usage"></a>cmake usage</h2>]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/content/2021/01/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>cudnn</title>
    <url>/content/2022/04/06/article/cudnn/</url>
    <content><![CDATA[<h1 id="cudnn-优化设置"><a href="#cudnn-优化设置" class="headerlink" title="cudnn 优化设置"></a>cudnn 优化设置</h1><h2 id="cudnn-deterministic"><a href="#cudnn-deterministic" class="headerlink" title="cudnn deterministic"></a>cudnn deterministic</h2><p>设置为true，cudnn使用非确定性算法，能够自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</p>
]]></content>
  </entry>
  <entry>
    <title>conv</title>
    <url>/content/2021/04/10/article/conv/</url>
    <content><![CDATA[<p>conv详细介绍。</p>
<h1 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inline bool IsExpand(const std::vector&lt;int64_t&gt;&amp; filter_dim,</span><br><span class="line">                     const std::vector&lt;int&gt;&amp; strides,</span><br><span class="line">                     const std::vector&lt;int&gt;&amp; paddings,</span><br><span class="line">                     const std::vector&lt;int&gt;&amp; dilations) &#123;</span><br><span class="line">  bool filter_1 &#x3D; true, strides_1 &#x3D; true, padding_0 &#x3D; true, dilation_1 &#x3D; true;</span><br><span class="line">  for (size_t j &#x3D; 0; j &lt; strides.size(); ++j) &#123;</span><br><span class="line">    filter_1 &#x3D; filter_1 &amp;&amp; (static_cast&lt;int&gt;(filter_dim[j + 2]) &#x3D;&#x3D; 1);</span><br><span class="line">    strides_1 &#x3D; strides_1 &amp;&amp; (strides[j] &#x3D;&#x3D; 1);</span><br><span class="line">    padding_0 &#x3D; padding_0 &amp;&amp; (paddings[j] &#x3D;&#x3D; 0);</span><br><span class="line">    dilation_1 &#x3D; dilation_1 &amp;&amp; (dilations[j] &#x3D;&#x3D; 1);</span><br><span class="line">  &#125;</span><br><span class="line">  return !(filter_1 &amp;&amp; strides_1 &amp;&amp; padding_0 &amp;&amp; dilation_1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; use col_shape in the im2col calculation</span><br><span class="line">&#x2F;&#x2F; col_shape_vec:</span><br><span class="line">&#x2F;&#x2F; &#123;i_c&#x2F;g, k_h, k_w, o_h, o_w&#125; or &#123;i_c&#x2F;g, k_d, k_h, k_w, o_d,o_h, o_w&#125;</span><br></pre></td></tr></table></figure>
<p>gemm calc</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; use col_matrix_shape in the gemm calculation size:</span><br><span class="line">&#x2F;&#x2F; (i_c&#x2F;g * k_h * k_w, o_h * o_w) or (i_c&#x2F;g * k_d * k_h * k_w, o_d * o_h * o_w)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static inline size_t naive_conv_out_size(size_t in_size, size_t pad,</span><br><span class="line">                                         size_t dilation, size_t ksize,</span><br><span class="line">                                         size_t stride) &#123;</span><br><span class="line">    return (in_size + 2 * pad - dilation * (ksize - 1) - 1) &#x2F; stride + 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class="line">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class="line">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class="line">                                       size_t px, size_t py, size_t sx,</span><br><span class="line">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class="line">    size_t oh &#x3D; naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class="line">    size_t ow &#x3D; naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class="line">    assert((group &gt;&#x3D; 1) &amp;&amp; (c % group &#x3D;&#x3D; 0) &amp;&amp; (k % group &#x3D;&#x3D; 0));</span><br><span class="line">    size_t k_per_group &#x3D; k &#x2F; group;</span><br><span class="line">    size_t c_per_group &#x3D; c &#x2F; group;</span><br><span class="line">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class="line">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class="line">    &#x2F;&#x2F; input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class="line">    for (ig &#x3D; 0; ig &lt; group; ig++) &#123;</span><br><span class="line">        for (in &#x3D; 0; in &lt; n; in++) &#123;</span><br><span class="line">            for (ik &#x3D; 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class="line">                for (ioh &#x3D; 0; ioh &lt; oh; ioh++) &#123;</span><br><span class="line">                    for (iow &#x3D; 0; iow &lt; ow; iow++) &#123;</span><br><span class="line">                        &#x2F;&#x2F; sliding window for this filter</span><br><span class="line">                        float value &#x3D; .0f;</span><br><span class="line">                        o_idx &#x3D; in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class="line">                        for (ic &#x3D; 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class="line">                            for (ir &#x3D; 0; ir &lt; fy; ir++) &#123;</span><br><span class="line">                                cur_h &#x3D; sy * ioh - py + dy * ir;</span><br><span class="line">                                if (cur_h &lt; 0 || cur_h &gt;&#x3D; h)</span><br><span class="line">                                    continue;</span><br><span class="line">                                for (is &#x3D; 0; is &lt; fx; is++) &#123;</span><br><span class="line">                                    cur_w &#x3D; sx * iow - px + dx * is;</span><br><span class="line">                                    if (cur_w &lt; 0 || cur_w &gt;&#x3D; w)</span><br><span class="line">                                        continue;</span><br><span class="line">                                    i_idx &#x3D; in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class="line">                                            cur_h * w + cur_w;</span><br><span class="line">                                    f_idx &#x3D; ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class="line">                                            ir * fx + is;</span><br><span class="line">                                    value +&#x3D; src[i_idx] * filter[f_idx];</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dst[o_idx] &#x3D; value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; group &#x3D; 1</span><br><span class="line">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class="line">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class="line">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class="line">                                       size_t px, size_t py, size_t sx,</span><br><span class="line">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class="line">    size_t oh &#x3D; naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class="line">    size_t ow &#x3D; naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class="line">    assert((group &gt;&#x3D; 1) &amp;&amp; (c % group &#x3D;&#x3D; 0) &amp;&amp; (k % group &#x3D;&#x3D; 0));</span><br><span class="line">    size_t k_per_group &#x3D; k &#x2F; group;</span><br><span class="line">    size_t c_per_group &#x3D; c &#x2F; group;</span><br><span class="line">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class="line">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class="line">    &#x2F;&#x2F; input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class="line">    for (ig &#x3D; 0; ig &lt; group; ig++) &#123;</span><br><span class="line">        for (in &#x3D; 0; in &lt; n; in++) &#123;</span><br><span class="line">            for (ik &#x3D; 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class="line">                for (ioh &#x3D; 0; ioh &lt; oh; ioh++) &#123;</span><br><span class="line">                    for (iow &#x3D; 0; iow &lt; ow; iow++) &#123;</span><br><span class="line">                        &#x2F;&#x2F; sliding window for this filter</span><br><span class="line">                        float value &#x3D; .0f;</span><br><span class="line">                        o_idx &#x3D; in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class="line">                        for (ic &#x3D; 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class="line">                            for (ir &#x3D; 0; ir &lt; fy; ir++) &#123;</span><br><span class="line">                                cur_h &#x3D; sy * ioh - py + dy * ir;</span><br><span class="line">                                if (cur_h &lt; 0 || cur_h &gt;&#x3D; h)</span><br><span class="line">                                    continue;</span><br><span class="line">                                for (is &#x3D; 0; is &lt; fx; is++) &#123;</span><br><span class="line">                                    cur_w &#x3D; sx * iow - px + dx * is;</span><br><span class="line">                                    if (cur_w &lt; 0 || cur_w &gt;&#x3D; w)</span><br><span class="line">                                        continue;</span><br><span class="line">                                    i_idx &#x3D; in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class="line">                                            cur_h * w + cur_w;</span><br><span class="line">                                    f_idx &#x3D; ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class="line">                                            ir * fx + is;</span><br><span class="line">                                    value +&#x3D; src[i_idx] * filter[f_idx];</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dst[o_idx] &#x3D; value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; [bs, ic, ih, iw] &amp; pack_size&#x3D;8 &#x3D;&gt; [bs, ic&#x2F;8, ih, iw, 8]</span><br><span class="line">&#x2F;&#x2F; [bs, ic, ih, iw] &amp; pack_size&#x3D;4 &#x3D;&gt; [bs, ic&#x2F;4, ih, iw, 4]</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; filter [oc, ic, kh, kw] &amp; pack_in&#x3D;8, pack_out&#x3D;8 &#x3D;&gt; [oc&#x2F;8, ic&#x2F;8, kh, kw, 8, 8]</span><br><span class="line">&#x2F;&#x2F; filter [oc, ic, kh, kw] &amp; pack_in&#x3D;4, pack_out&#x3D;4 &#x3D;&gt; [ic&#x2F;4, ic&#x2F;4, kh, kw, 4, 4]</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; [bs, ]</span><br></pre></td></tr></table></figure>
<h1 id="conv3d"><a href="#conv3d" class="headerlink" title="conv3d"></a>conv3d</h1><h1 id="conv-depthwise"><a href="#conv-depthwise" class="headerlink" title="conv_depthwise"></a>conv_depthwise</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; [bs, ic, ih, iw] &amp; pack_size&#x3D;8 &#x3D;&gt; [bs, ic&#x2F;8, ih, iw, 8]</span><br><span class="line">&#x2F;&#x2F; [bs, ic, ih, iw] &amp; pack_size&#x3D;4 &#x3D;&gt; [bs, ic&#x2F;4, ih, iw, 4]</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; filter [oc, ic&#x2F;groups&#x3D;1, kh, kw]</span><br><span class="line">&#x2F;&#x2F; filter [oc, 1, ih, iw] &amp; pack_size&#x3D;8 &#x3D;&gt; [oc&#x2F;8, ih, iw, 8]</span><br><span class="line">&#x2F;&#x2F; filter [oc, 1, ih, iw] &amp; pack_size&#x3D;4 &#x3D;&gt; [ic&#x2F;4, ih, iw, 4]</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; output [bs, oc, oh, ow]</span><br><span class="line">&#x2F;&#x2F; output_trans [bs, oc&#x2F;8, oh, ow, 8]</span><br><span class="line">&#x2F;&#x2F; output_trans [bs, oc&#x2F;4, oh, ow, 4]</span><br><span class="line">&#x2F;&#x2F; [bs, oc&#x2F;8, oh, ow, 8] &#x3D;&gt; [bs, oc, oh, ow]</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>daily note</title>
    <url>/content/2021/03/16/article/daily_note/</url>
    <content><![CDATA[<h2 id="gpu-code"><a href="#gpu-code" class="headerlink" title="gpu code"></a>gpu code</h2><p><a href="https://github.com/Oramy/m2-cgpu">https://github.com/Oramy/m2-cgpu</a></p>
<h2 id="dl-deploy"><a href="#dl-deploy" class="headerlink" title="dl deploy"></a>dl deploy</h2><p><a href="https://github.com/uber/neuropod">https://github.com/uber/neuropod</a></p>
<h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><p><a href="https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html">https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html</a></p>
<h2 id="gpu-resource"><a href="#gpu-resource" class="headerlink" title="gpu resource"></a>gpu resource</h2><ul>
<li>Arm Mali GPU Best Practices Developer Guide<br><a href="https://developer.arm.com/documentation/101897/latest">https://developer.arm.com/documentation/101897/latest</a></li>
<li>Arm Mali Bifrost and Valhall OpenCL Developer Guide<br><a href="https://developer.arm.com/documentation/101574/latest/">https://developer.arm.com/documentation/101574/latest/</a></li>
</ul>
<p><a href="https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/">https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/</a></p>
<h3 id="mail"><a href="#mail" class="headerlink" title="mail"></a>mail</h3><p>Midgard</p>
<p>write one 32-bit pixel per core per clock, 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle</p>
<p><img src="https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/8473.mali_2D00_top_2D00_level.png" alt="gpu arch"></p>
<p><img src="https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/1440.mali_2D00_top_2D00_core.png" alt="shader core"></p>
<h4 id="Tripipe-design"><a href="#Tripipe-design" class="headerlink" title="Tripipe design:"></a>Tripipe design:</h4><ul>
<li><p>arithmetic pipeline</p>
<p>simd向量处理引擎，作用于128 bit 4字的寄存器，可以有多个，一般是每个shader core两个。能够弹性访问的数据类型包括2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8。</p>
<p>OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle。</p>
<p>For Mali T604 and T628, peak performance is 17 FP32 FLOPS per ALU per cycle.</p>
<table>
<thead>
<tr>
<th>flops</th>
<th>instruction</th>
</tr>
</thead>
<tbody><tr>
<td>7</td>
<td>dot product (4 Muls, 3 adds)</td>
</tr>
<tr>
<td>1</td>
<td>scalar add</td>
</tr>
<tr>
<td>4</td>
<td>vec4 add</td>
</tr>
<tr>
<td>4</td>
<td>vec4 multiply</td>
</tr>
<tr>
<td>1</td>
<td>scalar multiply</td>
</tr>
</tbody></table>
<p>Mali-T760：600MHz，16 cores， 浮点计算性能为326 FP32 GFLOPS， 16 * 600M * 2 * 17 FP32 FLOPS。包含两个arithmetic pipeline，17 FP32 FLOPS per pipeline per clock cycle。</p>
</li>
<li><p>load/store pipeline</p>
</li>
<li><p>texture pipeline.</p>
<p>texuture访存，bilinear filtering 一个时钟周期，trilinear filtering从两个不同mipmaps memory加载，需要两个时钟周期</p>
</li>
</ul>
<h4 id="memory-system"><a href="#memory-system" class="headerlink" title="memory system"></a>memory system</h4><p>每个shader core包含两个16KB L1 数据cache，分别用于texture和常规数据访问。<br>一个逻辑L2 cache，所有的shader core共享，由厂商来配置，通常每个实例shader core 32KB或者64KB。</p>
<p><a href="https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals">https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals</a></p>
]]></content>
  </entry>
  <entry>
    <title>docker</title>
    <url>/content/2021/12/09/article/docker/</url>
    <content><![CDATA[<h1 id="docker-build"><a href="#docker-build" class="headerlink" title="docker build"></a>docker build</h1><p><a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a></p>
<h2 id="install-docker"><a href="#install-docker" class="headerlink" title="install docker"></a>install docker</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y yum-utils</span><br><span class="line">sudo yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br></pre></td></tr></table></figure>
<h1 id="ubuntu-docker"><a href="#ubuntu-docker" class="headerlink" title="ubuntu docker"></a>ubuntu docker</h1><p> <a href="https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04">https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM ubuntu:16.04</span><br><span class="line"></span><br><span class="line">LABEL com.zhangjun.image.authors&#x3D;&quot;ewalker.zj@gmail.com&quot;</span><br><span class="line"></span><br><span class="line">ENV TZ &quot;Asia&#x2F;Shanghai&quot;</span><br><span class="line"></span><br><span class="line">RUN apt update &amp;&amp; \</span><br><span class="line">    apt -qqy install software-properties-common &amp;&amp; \</span><br><span class="line">    add-apt-repository -y  ppa:ubuntu-toolchain-r&#x2F;test &amp;&amp; \</span><br><span class="line">    add-apt-repository -y ppa:deadsnakes&#x2F;ppa &amp;&amp; \</span><br><span class="line">    apt update &amp;&amp; \</span><br><span class="line">    apt -qqy install gcc-9 g++-9 &amp;&amp; \</span><br><span class="line">    apt -qqy install python3.7 &amp;&amp; \</span><br><span class="line">    update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;python python &#x2F;usr&#x2F;bin&#x2F;python3.7 10 &amp;&amp; \</span><br><span class="line">    update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;gcc gcc &#x2F;usr&#x2F;bin&#x2F;gcc-9 40 --slave &#x2F;usr&#x2F;bin&#x2F;g++ g++ &#x2F;usr&#x2F;bin&#x2F;g++-9 &amp;&amp; \</span><br><span class="line">    wget https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py &amp;&amp; \</span><br><span class="line">    python3.7 get-pip.py &amp;&amp; \</span><br><span class="line">    python3.7 -m pip install pre-commit &amp;&amp; \</span><br><span class="line">    apt-get -qqy clean &amp;&amp; \</span><br><span class="line">    rm -rf get-pip.py &amp;&amp; rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*</span><br><span class="line"></span><br><span class="line">#    update-alternatives --config gcc</span><br><span class="line">#    update-alternatives --config python</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>常用命令</title>
    <url>/content/2021/03/11/article/flags-1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>gflags</title>
    <url>/content/2021/03/11/article/flags/</url>
    <content><![CDATA[<p>主要介绍google gflags使用</p>
<h1 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h1><h2 id="定义flag"><a href="#定义flag" class="headerlink" title="定义flag"></a>定义flag</h2><p>一般.cc中定义flag，.h进行声明，其他包含该.h的文件就可以使用.cc定义的flag变量。</p>
<h2 id="flag与参数解析"><a href="#flag与参数解析" class="headerlink" title="flag与参数解析"></a>flag与参数解析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gflags::ParseCommandLineFlags(&amp;argc, &amp;argv, true); </span><br></pre></td></tr></table></figure>
<p>告诉程序处理命令行传入参数。最后一个参数为remove_flags，值为true，会移除相应flag和对应值并且修改argc值，argv只保留命令行参数；值为false，会保持argc不变，会调整argv中存储的内容顺序，flag放命令行参数前面。</p>
<h2 id="命令行设置flag"><a href="#命令行设置flag" class="headerlink" title="命令行设置flag"></a>命令行设置flag</h2><h2 id="更改flag默认值"><a href="#更改flag默认值" class="headerlink" title="更改flag默认值"></a>更改flag默认值</h2><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
  </entry>
  <entry>
    <title>hexo</title>
    <url>/content/2021/03/01/article/hexo/</url>
    <content><![CDATA[<h1 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h1><h1 id="hexo-init"><a href="#hexo-init" class="headerlink" title="hexo init"></a>hexo init</h1><p>该命令初始化博客</p>
<h2 id="new-post"><a href="#new-post" class="headerlink" title="new post"></a>new post</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new [layout] titile</span><br></pre></td></tr></table></figure>
<p>-p 自定义新文章路径, 如下面会在source/article目录下新建tensorrt.md</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new -p article&#x2F;tensorrt &quot;TensorRT&quot;</span><br></pre></td></tr></table></figure>
<h2 id="admin"><a href="#admin" class="headerlink" title="admin"></a>admin</h2><h2 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h2><p><a href="https://www.cnblogs.com/xingyunblog/p/8681205.html">hexo next后台管理</a></p>
]]></content>
  </entry>
  <entry>
    <title>MacTex</title>
    <url>/content/2022/03/02/article/mactex/</url>
    <content><![CDATA[<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><h2 id="macOS"><a href="#macOS" class="headerlink" title="macOS"></a>macOS</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install mactex </span><br></pre></td></tr></table></figure>

<h2 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt-get install texlive-xetex latex-cjk-all texmaker</span><br></pre></td></tr></table></figure>

<h1 id="link"><a href="#link" class="headerlink" title="link"></a>link</h1><p><a href="https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume">https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume</a><br><a href="https://github.com/billryan/resume/tree/zh_CN">https://github.com/billryan/resume/tree/zh_CN</a><br><a href="https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91">https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91</a></p>
]]></content>
  </entry>
  <entry>
    <title>Metal Basic</title>
    <url>/content/2021/09/05/article/metal_basic/</url>
    <content><![CDATA[<h1 id="metal-buffer-anc-texture"><a href="#metal-buffer-anc-texture" class="headerlink" title="metal buffer anc texture"></a>metal buffer anc texture</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">id&lt;MTLDevice&gt; device &#x3D; MTLCreateSystemDefaultDevice();</span><br><span class="line"></span><br><span class="line">MTLTextureDescriptor* desc &#x3D; [[MTLTextureDescriptor alloc] init];</span><br><span class="line">[desc setTextureType:MTLTextureType2DArray];</span><br><span class="line">[desc setDepth:1];</span><br><span class="line">desc.width &#x3D; static_cast&lt;NSUInteger&gt;(dim[2]);</span><br><span class="line">desc.height &#x3D; static_cast&lt;NSUInteger&gt;(dim[1]);</span><br><span class="line">desc.arrayLength &#x3D; static_cast&lt;NSUInteger&gt;(((dim[0]) * (dim[3]) + 3) &#x2F; 4);</span><br><span class="line">desc.pixelFormat &#x3D; MTLPixelFormatRGBA16Float;</span><br><span class="line">desc.usage &#x3D; MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;</span><br><span class="line">desc.storageMode &#x3D; MTLStorageModeShared;</span><br><span class="line"></span><br><span class="line">id&lt;MTLTexture&gt; image_ &#x3D; [device newTextureWithDescriptor:desc];</span><br><span class="line"></span><br><span class="line">int channels_per_pixel_ &#x3D; 4;</span><br><span class="line">int array_length_ &#x3D; desc_.arrayLength;</span><br><span class="line"></span><br><span class="line">auto count &#x3D; image_.width * image_.height * array_length_ * channels_per_pixel_;</span><br><span class="line">auto buffer &#x3D; static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class="line"></span><br><span class="line">auto bytes_per_row &#x3D; image_.width * image_.depth * channels_per_pixel_ * sizeof(uint16_t);</span><br><span class="line">auto bytes_per_image &#x3D; image_.height * bytes_per_row;</span><br><span class="line">const MTLRegion region &#123;</span><br><span class="line">    .origin &#x3D; &#123;0, 0, 0&#125;,</span><br><span class="line">    .size &#x3D;&#123;</span><br><span class="line">        image_.width, image_.height, image_.depth,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; copy from cpu to gpu</span><br><span class="line">for (int i &#x3D; 0; i &lt; array_length_; ++i) &#123;</span><br><span class="line">    auto p &#x3D; buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class="line">    [image_ replaceRegion:region</span><br><span class="line">              mipmapLevel:0</span><br><span class="line">                    slice:static_cast&lt;NSUInteger&gt;(i)</span><br><span class="line">                withBytes:p</span><br><span class="line">              bytesPerRow:bytes_per_row</span><br><span class="line">            bytesPerImage:bytes_per_image];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; copy from gpu to cpu</span><br><span class="line">auto* out_buffer &#x3D; static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class="line">for (int i &#x3D; 0; i &lt; array_length_; ++i) &#123;</span><br><span class="line">    auto p &#x3D; out_buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class="line"></span><br><span class="line">    [image_ getBytes:p</span><br><span class="line">         bytesPerRow:bytes_per_row</span><br><span class="line">       bytesPerImage:bytes_per_image</span><br><span class="line">          fromRegion:region</span><br><span class="line">         mipmapLevel:0</span><br><span class="line">               slice:static_cast&lt;NSUInteger&gt;(i)];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>swift </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">let device &#x3D; MTLCreateSystemDefaultDevice()!</span><br><span class="line">let queue &#x3D; device.makeCommandQueue()!</span><br><span class="line">let textureDescriptor &#x3D; MTLTextureDescriptor()</span><br><span class="line">textureDescriptor.textureType &#x3D; .type2D</span><br><span class="line">textureDescriptor.pixelFormat &#x3D; .r16Uint</span><br><span class="line">textureDescriptor.width &#x3D; bufferWidth</span><br><span class="line">textureDescriptor.height &#x3D; 256</span><br><span class="line">textureDescriptor.usage &#x3D; [.shaderRead, .shaderWrite]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">let texture &#x3D; buffer?.makeTexture(descriptor: textureDescriptor, offset: 0, bytesPerRow: bufferWidth*MemoryLayout&lt;UInt16&gt;.stride)</span><br><span class="line"></span><br><span class="line">let texture &#x3D; device.makeTexture(descriptor: textureDescriptor)</span><br><span class="line">texture?.replace(region: MTLRegionMake2D(0, 0, w, h), mipmapLevel: 0, withBytes: data, bytesPerRow: 4 * w)</span><br><span class="line"></span><br><span class="line"># buffer</span><br><span class="line">let count &#x3D; 1500</span><br><span class="line">var myVector &#x3D; [Float](repeating: 0, count: count)</span><br><span class="line">var length &#x3D; count * MemoryLayout&lt; Float &gt;.stride</span><br><span class="line">var outBuffer &#x3D; device.makeBuffer(bytes: myVector, length: length, options: [])</span><br><span class="line">for (index, value) in myVector.enumerated() &#123; myVector[index] &#x3D; Float(index) &#125;</span><br><span class="line">var inBuffer &#x3D; device.makeBuffer(bytes: myVector, length: length, options: [])</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>MNN</title>
    <url>/content/2021/06/07/article/mnn/</url>
    <content><![CDATA[<p>本文主要介绍Alibaba MNN编译使用。</p>
<h1 id="下载编译"><a href="#下载编译" class="headerlink" title="下载编译"></a>下载编译</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone github.com/alibaba/MNN</span><br></pre></td></tr></table></figure>
<h2 id="linux-x86"><a href="#linux-x86" class="headerlink" title="linux x86"></a>linux x86</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;schema&#x2F;generate.sh</span><br><span class="line">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make -j4</span><br></pre></td></tr></table></figure>
<p>refer to <code>https://www.yuque.com/mnn/en/build_linux</code></p>
<h1 id="模型部署示例"><a href="#模型部署示例" class="headerlink" title="模型部署示例"></a>模型部署示例</h1><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> main();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>models</title>
    <url>/content/2021/03/23/article/models/</url>
    <content><![CDATA[<p>介绍paddle、ncnn、tnn使用</p>
<h1 id="paddle"><a href="#paddle" class="headerlink" title="paddle"></a>paddle</h1><h2 id="x2paddle"><a href="#x2paddle" class="headerlink" title="x2paddle"></a>x2paddle</h2><p>x2paddle –framework=onnx –model=onnx_model.onnx –save_dir=mobilenet</p>
<h2 id="paddle2onnx"><a href="#paddle2onnx" class="headerlink" title="paddle2onnx"></a>paddle2onnx</h2><p>paddle2onnx –model_dir paddle_model  –save_file onnx_file –opset_version 10 –enable_onnx_checker True<br>paddle2onnx –model_dir paddle_model  –model_filename model_filename –params_filename params_filename –save_file onnx_file –opset_version 10 –enable_onnx_checker True</p>
]]></content>
  </entry>
  <entry>
    <title>Modern Cpp</title>
    <url>/content/2022/01/09/article/modern_cpp/</url>
    <content><![CDATA[<h1 id="cpp11"><a href="#cpp11" class="headerlink" title="cpp11"></a>cpp11</h1><h2 id="type-traits"><a href="#type-traits" class="headerlink" title="type traits"></a>type traits</h2><ul>
<li><p>std::integral_constant</p>
<p>  wrap a static constant of specified type. Defined in <type_traits></p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">template&lt;class T, T v&gt;</span><br><span class="line">struct integral_constant;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>notes</title>
    <url>/content/2021/03/12/article/notes/</url>
    <content><![CDATA[<p>一些记录</p>
<h1 id="ccache和distcc"><a href="#ccache和distcc" class="headerlink" title="ccache和distcc"></a>ccache和distcc</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export USE_CCACHE&#x3D;1</span><br><span class="line">export CCACHE_DIR&#x3D;&#x2F;home&#x2F;xx&#x2F;tools&#x2F;.ccache</span><br><span class="line">ccache -M 50G</span><br><span class="line">ccache -s</span><br><span class="line">ccache -C</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake中使用ccache的最加方案：cmake &gt; 3.5，命令行上-DCMAKE_CXX_COMPILER_LAUNCHER&#x3D;ccache配置文件</span><br><span class="line">find_program(CCACHE_FOUND ccache)</span><br><span class="line">    if(CCACHE_FOUND)  </span><br><span class="line">        set(CMAKE_CXX_COMPILER_LAUNCHER ccache)</span><br><span class="line">    endif()</span><br></pre></td></tr></table></figure>

<h1 id="conv"><a href="#conv" class="headerlink" title="conv"></a>conv</h1><table>
<thead>
<tr>
<th>input</th>
<th>filter</th>
<th>output</th>
</tr>
</thead>
<tbody><tr>
<td>1x32x40x80</td>
<td>16x32x3x3</td>
<td>1x16x40x80</td>
</tr>
</tbody></table>
<p>int kernel_size = kernel_w * kernel_h;<br>int num_input = weight_data_size / kernel_size / num_output;</p>
<h1 id="cento8-epel配置aliyun源"><a href="#cento8-epel配置aliyun源" class="headerlink" title="cento8 epel配置aliyun源"></a>cento8 epel配置aliyun源</h1><p>首先安装epel配置包<br>yum install -y  <a href="https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm">https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm</a><br>然后将 repo 配置中的地址替换为阿里云镜像站地址<br>sed -i ‘s|^#baseurl=<a href="https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|&#39;">https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|&#39;</a> /etc/yum.repos.d/epel*<br>sed -i ‘s|^metalink|#metalink|’ /etc/yum.repos.d/epel*</p>
]]></content>
  </entry>
  <entry>
    <title>Nsight System</title>
    <url>/content/2022/03/17/article/nsight-system/</url>
    <content><![CDATA[<h1 id="Nsight-System"><a href="#Nsight-System" class="headerlink" title="Nsight System"></a>Nsight System</h1><p>下载地址：<a href="https://developer.nvidia.com/gameworksdownload#?search=nsight">https://developer.nvidia.com/gameworksdownload#?search=nsight</a></p>
<h1 id="Nsight-Compute"><a href="#Nsight-Compute" class="headerlink" title="Nsight Compute"></a>Nsight Compute</h1>]]></content>
  </entry>
  <entry>
    <title>OpenCL</title>
    <url>/content/2022/03/06/article/opencl/</url>
    <content><![CDATA[<p><img src="https://pic2.zhimg.com/80/v2-a7526bdfdb0c6372745f272d6f315291_1440w.jpg" alt="opencl"><br><img src="https://pic2.zhimg.com/80/v2-2ce3ca6ae987befcc7bfb9f3360e0acd_1440w.jpg" alt="opencl"></p>
<h1 id="OpenCL平台模型"><a href="#OpenCL平台模型" class="headerlink" title="OpenCL平台模型"></a>OpenCL平台模型</h1><p>fiber(work item) - wave - workgroup</p>
<p>(thread - warp - block?)</p>
<h1 id="OpenCL执行模型"><a href="#OpenCL执行模型" class="headerlink" title="OpenCL执行模型"></a>OpenCL执行模型</h1><h2 id="上下文"><a href="#上下文" class="headerlink" title="上下文"></a>上下文</h2><h2 id="命令队列"><a href="#命令队列" class="headerlink" title="命令队列"></a>命令队列</h2><h2 id="kernel执行"><a href="#kernel执行" class="headerlink" title="kernel执行"></a>kernel执行</h2><h1 id="OpenCL存储器模型"><a href="#OpenCL存储器模型" class="headerlink" title="OpenCL存储器模型"></a>OpenCL存储器模型</h1><h2 id="存储类型"><a href="#存储类型" class="headerlink" title="存储类型"></a>存储类型</h2><ul>
<li>host memory</li>
<li>global memory</li>
<li>constant memory<br>片内延迟低，系统RAM延迟高。work group中所有work item的常量数据。</li>
<li>local memory<br>一个work group内的所有work item共享。<br>Local Memory coalesced access<br><img src="https://pic3.zhimg.com/80/v2-4ab02630d7f1fdc1d01ce2973316788e_1440w.jpg" alt="coalesced access"></li>
<li>private memory</li>
</ul>
<h2 id="存储对象类型"><a href="#存储对象类型" class="headerlink" title="存储对象类型"></a>存储对象类型</h2><ul>
<li>buffer</li>
<li>image</li>
<li>pipe</li>
</ul>
<h1 id="OpenCL-API"><a href="#OpenCL-API" class="headerlink" title="OpenCL API"></a>OpenCL API</h1><p>clCreateProgramWithSource()<br>clBuildProgram()<br>clLinkProgram()<br>clUnloadPlatformCompiler()<br>clCreateProgramWithBinary()</p>
<p>clCreate{Image|Buffer}<br>clEnqueueNDRangeKernel()</p>
<p>cl_mem clCreateBuffer (<br>    cl_context context,<br>    cl_mem_flags flags,<br>    size_t size,<br>    void *host_ptr,<br>    cl_int *errcode_ret)</p>
<h1 id="OpenCL性能优化"><a href="#OpenCL性能优化" class="headerlink" title="OpenCL性能优化"></a>OpenCL性能优化</h1><h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><ul>
<li>local memory<br>不同work item之间需要barrier进行同步，操作耗时。</li>
</ul>
<p>不同work item之间交换数据需要barrier进行同步。</p>
<p>Barrier 经常会导致同步延迟，从而阻塞ALU，导致更低的ALU的使用效率。</p>
<p>在某些情况下，将数据缓冲到本地内存中可能会需要同步，同步产生的延迟将会抵消使用本地内存带来的性能提升。在这种情况下，直接使用全局内存，避免使用barrier可能是更好的选择。</p>
<h1 id="OpenCL-资料"><a href="#OpenCL-资料" class="headerlink" title="OpenCL 资料"></a>OpenCL 资料</h1><p><a href="https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf">https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf</a><br><a href="https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html">https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html</a><br><a href="https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization">https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization</a></p>
<p><a href="https://www.cnblogs.com/xiajingwang/p/11120561.html">Qualcomm_Mobile_OpenCL 中文翻译</a></p>
]]></content>
  </entry>
  <entry>
    <title>paddle inference</title>
    <url>/content/2021/03/16/article/paddle_inference/</url>
    <content><![CDATA[<p>paddle inference学习记录</p>
<h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><p>paddle inference代码位于paddle/fluid/inference下面。</p>
<h2 id="engine基类"><a href="#engine基类" class="headerlink" title="engine基类"></a>engine基类</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class EngineBase &#123;</span><br><span class="line"> public:</span><br><span class="line">  using DescType &#x3D; ::paddle::framework::proto::BlockDesc;</span><br><span class="line">  &#x2F;&#x2F; Build the model and do some preparation, for example, in TensorRT, run</span><br><span class="line">  &#x2F;&#x2F; createInferBuilder, buildCudaEngine.</span><br><span class="line">  virtual void Build(const DescType&amp; paddle_model) &#x3D; 0;</span><br><span class="line">  &#x2F;&#x2F; Execute the engine, that will run the inference network.</span><br><span class="line">  virtual void Execute(int batch_size) &#x3D; 0;</span><br><span class="line">  virtual ~EngineBase() &#123;&#125;</span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>

<h2 id="待添加"><a href="#待添加" class="headerlink" title="待添加"></a>待添加</h2><p>framework::ProgramDesc<br>framework::Executor* executor<br>framework::Scope* scope</p>
<h1 id="paddle-inference-执行逻辑"><a href="#paddle-inference-执行逻辑" class="headerlink" title="paddle inference 执行逻辑"></a>paddle inference 执行逻辑</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">paddle&#x2F;fluid&#x2F;framework&#x2F;naive_executor.cc#L41</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void NaiveExecutor::Run() &#123;</span><br><span class="line">#ifdef PADDLE_WITH_MKLDNN</span><br><span class="line">  platform::AttachPointerHashToMKLDNNKey(this, place_);</span><br><span class="line">  platform::RegisterModelLayout(ops_, place_);</span><br><span class="line">#endif</span><br><span class="line">  platform::ScopedFlushDenormal flush;</span><br><span class="line">  for (auto &amp;op : ops_) &#123;</span><br><span class="line">    VLOG(4) &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot; run &quot;</span><br><span class="line">            &lt;&lt; op-&gt;DebugStringEx(scope_) &lt;&lt; &quot; on scope &quot; &lt;&lt; scope_;</span><br><span class="line">    op-&gt;SetIsCalledByExecutor(false);</span><br><span class="line">    op-&gt;Run(*scope_, place_);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">paddle&#x2F;fluid&#x2F;framework&#x2F;operator.cc#L204</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void OperatorBase::Run(const Scope&amp; scope, const platform::Place&amp; place) &#123;</span><br><span class="line">  auto dev_id &#x3D; place.device;</span><br><span class="line">  platform::SetDeviceId(dev_id);</span><br><span class="line">  auto op_name &#x3D; platform::OpName(outputs_, Type());</span><br><span class="line">  RunImpl(scope, place);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void OperatorWithKernel::RunImpl(const Scope&amp; scope,</span><br><span class="line">                                 const platform::Place&amp; place,</span><br><span class="line">                                 RuntimeContext* runtime_ctx) const &#123;</span><br><span class="line">  platform::DeviceContextPool&amp; pool &#x3D; platform::DeviceContextPool::Instance();</span><br><span class="line">  auto* dev_ctx &#x3D; pool.Get(place);</span><br><span class="line">  auto exe_ctx &#x3D; ExecutionContext(*this, scope, *dev_ctx, *runtime_ctx);</span><br><span class="line">  &#x2F;&#x2F; using cache</span><br><span class="line">  if (kernel_type_.get()) &#123;</span><br><span class="line">    dev_ctx &#x3D; pool.Get(kernel_type_-&gt;place_);</span><br><span class="line">  &#125;</span><br><span class="line">  &#123;</span><br><span class="line">    impl_ &#x3D;</span><br><span class="line">        new CacheImpl(new phi::KernelContext(),</span><br><span class="line">                          new RuntimeInferShapeContext(*this, *runtime_ctx));</span><br><span class="line">    BuildPhiKernelContext(*runtime_ctx, dev_ctx, impl_-&gt;getKernelContext());</span><br><span class="line"></span><br><span class="line">    (*pt_kernel_)(impl_-&gt;getKernelContext());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>Metal for Paddle Lite</title>
    <url>/content/2022/01/05/article/paddlelite-metal/</url>
    <content><![CDATA[<h1 id="Metal-for-Paddle-Lite"><a href="#Metal-for-Paddle-Lite" class="headerlink" title="Metal for Paddle Lite"></a>Metal for Paddle Lite</h1><p><img src="https://user-images.githubusercontent.com/1312389/161764131-2e745999-bc96-4c92-9bb0-1e459bc46c95.png" alt="image"></p>
<ul>
<li>Metal kernel and context<br><img src="https://user-images.githubusercontent.com/1312389/161764543-9e53e60a-6dbb-4c28-9d32-b23b41857b09.png" alt="image"></li>
<li>Metal OP executation<br><img src="https://user-images.githubusercontent.com/1312389/161764633-cfc0e0ca-1786-48ad-97b5-4e8410dc8a05.png" alt="image"></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Paddle Lite framework</title>
    <url>/content/2021/09/19/article/paddle-lite/</url>
    <content><![CDATA[<h1 id="core"><a href="#core" class="headerlink" title="core"></a>core</h1><h2 id="context"><a href="#context" class="headerlink" title="context"></a>context</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class KernelContext &#123;</span><br><span class="line"> public:</span><br><span class="line">  template &lt;typename ContextT&gt;</span><br><span class="line">  ContextT&amp; As() &#123;</span><br><span class="line">    if (!ctx_.valid()) &#123;</span><br><span class="line">      ctx_.set&lt;ContextT&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    return *ctx_.get_mutable&lt;ContextT&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  Any ctx_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ContextScheduler &#123;</span><br><span class="line"> public:</span><br><span class="line">  static ContextScheduler&amp; Global() &#123;</span><br><span class="line">    static auto* x &#x3D; new ContextScheduler;</span><br><span class="line">    return *x;</span><br><span class="line">  &#125;</span><br><span class="line">  std::unique_ptr&lt;KernelContext&gt; NewContext(</span><br><span class="line">      TargetType target,</span><br><span class="line">      &#x2F;*only used for cuda context*&#x2F; int exec_stream_id &#x3D; 0) &#123;</span><br><span class="line">    std::unique_ptr&lt;KernelContext&gt; ctx(new KernelContext);</span><br><span class="line">    switch (target) &#123;</span><br><span class="line">      case TARGET(kHost):</span><br><span class="line">        kernel_contexts_[TargetType::kHost].As&lt;HostContext&gt;().CopySharedTo(</span><br><span class="line">            &amp;ctx-&gt;As&lt;HostContext&gt;());</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line">    return ctx;</span><br><span class="line">  &#125; </span><br><span class="line">private:</span><br><span class="line">  template &lt;TargetType Type, typename ContextT&gt;</span><br><span class="line">  void InitContext() &#123;</span><br><span class="line">    kernel_contexts_[Type].As&lt;ContextT&gt;().InitOnce();</span><br><span class="line">  &#125;</span><br><span class="line">  ContextScheduler() &#123;</span><br><span class="line">    InitContext&lt;TargetType::kHost, HostContext&gt;();</span><br><span class="line">  &#125;</span><br><span class="line">private:</span><br><span class="line">  std::map&lt;TargetType, KernelContext&gt; kernel_contexts_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="op-lite"><a href="#op-lite" class="headerlink" title="op lite"></a>op lite</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class OpLite : public Registry &#123;</span><br><span class="line"> public:</span><br><span class="line">  OpLite() &#x3D; default;</span><br><span class="line">  explicit OpLite(const std::string &amp;type) : op_type_(type) &#123;&#125;</span><br><span class="line">  explicit OpLite(const std::vector&lt;Place&gt; &amp;valid_places)</span><br><span class="line">      : valid_places_(valid_places) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  void SetValidPlaces(const std::vector&lt;Place&gt; &amp;places) &#123;</span><br><span class="line">    VLOG(5) &lt;&lt; &quot;valid places &quot; &lt;&lt; valid_places_.size();</span><br><span class="line">    valid_places_ &#x3D; places;</span><br><span class="line">  &#125;</span><br><span class="line">  virtual bool Run();</span><br><span class="line">  &#x2F;&#x2F; Indicate whether the Op runs only once or not</span><br><span class="line">  virtual bool run_once() const &#123; return false; &#125;</span><br><span class="line">  std::string Type() const &#123; return op_type_; &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Link the external execution environ to internal context.</span><br><span class="line">  bool Attach(const cpp::OpDesc &amp;opdesc, lite::Scope *scope);</span><br><span class="line"></span><br><span class="line">  template &lt;typename T&gt;</span><br><span class="line">  inline void AttachParam(T *param) &#123;</span><br><span class="line">    op_param_ &#x3D; static_cast&lt;T *&gt;(param);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F; Create all the kernels for the valid targets.</span><br><span class="line">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; CreateKernels(</span><br><span class="line">      const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type &#x3D; &quot;&quot;);</span><br><span class="line"></span><br><span class="line">  Scope *scope() &#123; return scope_; &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Assign op param to kernel.</span><br><span class="line">  virtual void AttachKernel(KernelBase *kernel) &#x3D; 0;</span><br><span class="line">  void SetKernel(std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; &amp;kernels) &#123;  &#x2F;&#x2F; NOLINT</span><br><span class="line">    kernel_ &#x3D; std::move(kernels.front());</span><br><span class="line">    kernel_-&gt;SetContext(</span><br><span class="line">        ContextScheduler::Global().NewContext(kernel_-&gt;target()));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  KernelBase *GetKernel() &#123;  &#x2F;&#x2F; NOLINT</span><br><span class="line">    return kernel_.get();</span><br><span class="line">  &#125;</span><br><span class="line">  virtual ~OpLite() &#x3D; default;</span><br><span class="line"> protected:</span><br><span class="line">  friend class mir::Node;</span><br><span class="line">  friend class mir::SSAGraph;</span><br><span class="line"> protected:</span><br><span class="line">  Scope *scope_&#123;nullptr&#125;;</span><br><span class="line">  std::unique_ptr&lt;KernelBase&gt; kernel_;</span><br><span class="line">  std::string op_type_;</span><br><span class="line">  std::vector&lt;Place&gt; valid_places_;</span><br><span class="line">  Place kernel_place_&#123;TARGET(kHost), PRECISION(kFloat)&#125;;</span><br><span class="line">  std::unique_ptr&lt;OpInfo&gt; op_info_;</span><br><span class="line">  &#x2F;&#x2F; todo: it&#39;s prefered to combine last_input_shapes and</span><br><span class="line">  &#x2F;&#x2F; last_input_lods into a single hash value to decrease</span><br><span class="line">  &#x2F;&#x2F; memory usage.</span><br><span class="line">  std::vector&lt;DDimLite&gt; last_input_shapes&#123;&#125;;</span><br><span class="line">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_input_lods&#123;&#125;;</span><br><span class="line">  std::vector&lt;DDimLite&gt; last_output_shapes&#123;&#125;;</span><br><span class="line">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_output_lods&#123;&#125;;</span><br><span class="line">  mutable operators::ParamBase *op_param_&#123;nullptr&#125;;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  &#x2F;&#x2F; Infer Shape according to memory, if current input shapes are consistent</span><br><span class="line">  &#x2F;&#x2F; with that of previous inputs, output shapes of last time will be reused.</span><br><span class="line">  bool InferShapeWithCache();</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; OpLite::CreateKernels(</span><br><span class="line">    const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type) &#123;</span><br><span class="line">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; kernels;</span><br><span class="line">  CHECK(!op_type_.empty()) &lt;&lt; &quot;op_type_ should be set first&quot;;</span><br><span class="line"></span><br><span class="line">  auto pick_kernel &#x3D; [&amp;](const Place &amp;place) &#123;</span><br><span class="line">    auto ks &#x3D; KernelRegistry::Global().Create(</span><br><span class="line">        op_type_, place.target, place.precision, place.layout);</span><br><span class="line">    VLOG(5) &lt;&lt; &quot;pick kernel for &quot; &lt;&lt; op_info()-&gt;Type() &lt;&lt; &quot; &quot;</span><br><span class="line">            &lt;&lt; place.DebugString() &lt;&lt; &quot; get &quot; &lt;&lt; ks.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class="line">    for (auto &amp;&amp;it : ks) &#123;</span><br><span class="line">      AttachKernel(it.get());</span><br><span class="line">      kernels.emplace_back(std::move(it));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  if (!kernel_type.empty()) &#123;</span><br><span class="line">    Place place;</span><br><span class="line">    std::string op_type, alias;</span><br><span class="line">    KernelBase::ParseKernelType(kernel_type, &amp;op_type, &amp;alias, &amp;place);</span><br><span class="line">    pick_kernel(place);</span><br><span class="line">    CHECK(!kernels.empty()) &lt;&lt; &quot;no kernel for kernel type &quot; &lt;&lt; kernel_type;</span><br><span class="line">    return kernels;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::set&lt;Place&gt; expanded_places(places.begin(), places.end());</span><br><span class="line">  for (auto &amp;place : places) &#123;</span><br><span class="line">    &#x2F;&#x2F; Pick kernels those support any Precision and any DataLayout, For example:</span><br><span class="line">    &#x2F;&#x2F; kARM,kFloat,kNCHW -&gt; kARM,kFloat,kAny; kARM,kAny,kNCHW; kARM,kAny,kAny</span><br><span class="line">    expanded_places.insert(</span><br><span class="line">        Place(place.target, place.precision, DATALAYOUT(kAny)));</span><br><span class="line">    expanded_places.insert(Place(place.target, PRECISION(kAny), place.layout));</span><br><span class="line">    expanded_places.insert(</span><br><span class="line">        Place(place.target, PRECISION(kAny), DATALAYOUT(kAny)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::set&lt;TargetType&gt; targets;</span><br><span class="line">  for (auto place : expanded_places) &#123;</span><br><span class="line">    pick_kernel(place);</span><br><span class="line">    targets.insert(place.target);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  VLOG(5) &lt;&lt; &quot;op &quot; &lt;&lt; op_type_ &lt;&lt; &quot; get &quot; &lt;&lt; kernels.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class="line">  return kernels;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * Operator Information, such as some description. It will be shared by all the</span><br><span class="line"> * kernels of the same operator.</span><br><span class="line"> *&#x2F;</span><br><span class="line">class OpInfo : public cpp::OpDesc &#123;</span><br><span class="line"> public:</span><br><span class="line">  OpInfo(const OpInfo &amp;) &#x3D; default;</span><br><span class="line">  explicit OpInfo(const cpp::OpDesc &amp;other) : cpp::OpDesc(other) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="op-registry"><a href="#op-registry" class="headerlink" title="op registry"></a>op registry</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class OpKernelInfoCollector &#123;</span><br><span class="line"> public:</span><br><span class="line">  static OpKernelInfoCollector&amp; Global() &#123;</span><br><span class="line">    static auto* x &#x3D; new OpKernelInfoCollector;</span><br><span class="line">    return *x;</span><br><span class="line">  &#125;</span><br><span class="line">  void AddOp2path(const std::string&amp; op_name, const std::string&amp; op_path);</span><br><span class="line">  void AddKernel2path(const std::string&amp; kernel_name,</span><br><span class="line">                      const std::string&amp; kernel_path);</span><br><span class="line">  </span><br><span class="line"> private:</span><br><span class="line">  std::map&lt;std::string, std::string&gt; op2path_;</span><br><span class="line">  std::map&lt;std::string, std::string&gt; kernel2path_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class OpLiteFactory &#123;</span><br><span class="line"> public:</span><br><span class="line">  &#x2F;&#x2F; Register a function to create an op</span><br><span class="line">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class="line">                       std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class="line">    op_registry_[op_type] &#x3D; fun;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static OpLiteFactory&amp; Global() &#123;</span><br><span class="line">    static OpLiteFactory* x &#x3D; new OpLiteFactory;</span><br><span class="line">    return *x;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::shared_ptr&lt;OpLite&gt; Create(const std::string&amp; op_type) const &#123;</span><br><span class="line">    auto it &#x3D; op_registry_.find(op_type);</span><br><span class="line">    if (it &#x3D;&#x3D; op_registry_.end()) return nullptr;</span><br><span class="line">    return it-&gt;second();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::string DebugString();</span><br><span class="line"></span><br><span class="line">  std::vector&lt;std::string&gt; GetAllOps() const &#123;</span><br><span class="line">    std::vector&lt;std::string&gt; res;</span><br><span class="line">    for (const auto&amp; op : op_registry_) &#123;</span><br><span class="line">      res.push_back(op.first);</span><br><span class="line">    &#125;</span><br><span class="line">    return res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> protected:</span><br><span class="line">  std::map&lt;std::string, std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt;&gt; op_registry_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class OpLiteRegistrar &#123;</span><br><span class="line"> public:</span><br><span class="line">  OpLiteRegistrar(const std::string&amp; op_type,</span><br><span class="line">                  std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class="line">    OpLiteFactory::Global().RegisterCreator(op_type, fun);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F; Touch function is used to guarantee registrar was initialized.</span><br><span class="line">  void touch() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">class KernelFactory &#123;</span><br><span class="line"> public:</span><br><span class="line">  &#x2F;&#x2F; Register a function to create kernels</span><br><span class="line">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class="line">                       TargetType target,</span><br><span class="line">                       PrecisionType precision,</span><br><span class="line">                       DataLayoutType layout,</span><br><span class="line">                       std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class="line">    op_registry_[op_type][std::make_tuple(target, precision, layout)].push_back(</span><br><span class="line">        fun);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static KernelFactory&amp; Global() &#123;</span><br><span class="line">    static KernelFactory* x &#x3D; new KernelFactory;</span><br><span class="line">    return *x;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Create all kernels belongs to an op.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type) &#123;</span><br><span class="line">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class="line">    if (op_registry_.find(op_type) &#x3D;&#x3D; op_registry_.end()) return res;</span><br><span class="line">    auto&amp; kernel_registry &#x3D; op_registry_[op_type];</span><br><span class="line">    for (auto it &#x3D; kernel_registry.begin(); it !&#x3D; kernel_registry.end(); ++it) &#123;</span><br><span class="line">      for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class="line">        res.emplace_back(fun());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Create a specific kernel. Return a list for API compatible.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type,</span><br><span class="line">                                                TargetType target,</span><br><span class="line">                                                PrecisionType precision,</span><br><span class="line">                                                DataLayoutType layout) &#123;</span><br><span class="line">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class="line">    if (op_registry_.find(op_type) &#x3D;&#x3D; op_registry_.end()) return res;</span><br><span class="line">    auto&amp; kernel_registry &#x3D; op_registry_[op_type];</span><br><span class="line">    auto it &#x3D; kernel_registry.find(std::make_tuple(target, precision, layout));</span><br><span class="line">    if (it &#x3D;&#x3D; kernel_registry.end()) return res;</span><br><span class="line">    for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class="line">      res.emplace_back(fun());</span><br><span class="line">    &#125;</span><br><span class="line">    return res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> protected:</span><br><span class="line">  &#x2F;&#x2F; Outer map: op -&gt; a map of kernel.</span><br><span class="line">  &#x2F;&#x2F; Inner map: kernel -&gt; creator function.</span><br><span class="line">  &#x2F;&#x2F; Each kernel was represented by a combination of &lt;TargetType, PrecisionType,</span><br><span class="line">  &#x2F;&#x2F; DataLayoutType&gt;</span><br><span class="line">  std::map&lt;std::string,</span><br><span class="line">           std::map&lt;std::tuple&lt;TargetType, PrecisionType, DataLayoutType&gt;,</span><br><span class="line">                    std::list&lt;std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt;&gt;&gt;&gt;</span><br><span class="line">      op_registry_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Register Kernel by initializing a static KernelRegistrar instance</span><br><span class="line">class KernelRegistrar &#123;</span><br><span class="line"> public:</span><br><span class="line">  KernelRegistrar(const std::string&amp; op_type,</span><br><span class="line">                  TargetType target,</span><br><span class="line">                  PrecisionType precision,</span><br><span class="line">                  DataLayoutType layout,</span><br><span class="line">                  std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class="line">    KernelFactory::Global().RegisterCreator(</span><br><span class="line">        op_type, target, precision, layout, fun);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F; Touch function is used to guarantee registrar was initialized.</span><br><span class="line">  void touch() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">class ParamTypeDummyRegistry &#123;</span><br><span class="line"> public:</span><br><span class="line">  struct NewInstance &#123;</span><br><span class="line">    NewInstance() &#123;&#125;</span><br><span class="line">    NewInstance&amp; BindInput(const std::string&amp; arg_name,</span><br><span class="line">                           const ParamType&amp; ptype) &#123;</span><br><span class="line">      return *this;</span><br><span class="line">    &#125;</span><br><span class="line">    NewInstance&amp; BindOutput(const std::string&amp; arg_name,</span><br><span class="line">                            const ParamType&amp; ptype) &#123;</span><br><span class="line">      return *this;</span><br><span class="line">    &#125;</span><br><span class="line">    NewInstance&amp; SetVersion(const std::string&amp; version) &#123; return *this; &#125;</span><br><span class="line">    NewInstance&amp; BindPaddleOpVersion(const std::string&amp; op_type,</span><br><span class="line">                                     int32_t version_id) &#123;</span><br><span class="line">      return *this;</span><br><span class="line">    &#125;</span><br><span class="line">    bool Finalize() &#123; return true; &#125;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  ParamTypeDummyRegistry() &#x3D; default;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="注册机制"><a href="#注册机制" class="headerlink" title="注册机制"></a>注册机制</h1><h2 id="op注册"><a href="#op注册" class="headerlink" title="op注册"></a>op注册</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#define REGISTER_LITE_OP(op_type__, OpClass)                                   \</span><br><span class="line">  static paddle::lite::OpLiteRegistrar op_type__##__registry(                  \</span><br><span class="line">      #op_type__, []() &#123;                                                       \</span><br><span class="line">        return std::unique_ptr&lt;paddle::lite::OpLite&gt;(new OpClass(#op_type__)); \</span><br><span class="line">      &#125;);                                                                      \</span><br><span class="line">  int touch_op_##op_type__() &#123;                                                 \</span><br><span class="line">    op_type__##__registry.touch();                                             \</span><br><span class="line">    OpKernelInfoCollector::Global().AddOp2path(#op_type__, __FILE__);          \</span><br><span class="line">    return 0;                                                                  \</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="kernel-注册"><a href="#kernel-注册" class="headerlink" title="kernel 注册"></a>kernel 注册</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Register a kernel.</span><br><span class="line">#define REGISTER_LITE_KERNEL(                                                 \</span><br><span class="line">    op_type__, target__, precision__, layout__, KernelClass, alias__)         \</span><br><span class="line">  static paddle::lite::KernelRegistrar                                        \</span><br><span class="line">      op_type__##target__##precision__##layout__##alias__##_kernel_registry(  \</span><br><span class="line">          #op_type__,                                                         \</span><br><span class="line">          TARGET(target__),                                                   \</span><br><span class="line">          PRECISION(precision__),                                             \</span><br><span class="line">          DATALAYOUT(layout__),                                               \</span><br><span class="line">          []() &#123;                                                              \</span><br><span class="line">            std::unique_ptr&lt;KernelClass&gt; x(new KernelClass);                  \</span><br><span class="line">            x-&gt;set_op_type(#op_type__);                                       \</span><br><span class="line">            x-&gt;set_alias(#alias__);                                           \</span><br><span class="line">            return x;                                                         \</span><br><span class="line">          &#125;);                                                                 \</span><br><span class="line">  int touch_##op_type__##target__##precision__##layout__##alias__() &#123;         \</span><br><span class="line">    op_type__##target__##precision__##layout__##alias__##_kernel_registry     \</span><br><span class="line">        .touch();                                                             \</span><br><span class="line">    OpKernelInfoCollector::Global().AddKernel2path(                           \</span><br><span class="line">        #op_type__ &quot;,&quot; #target__ &quot;,&quot; #precision__ &quot;,&quot; #layout__ &quot;,&quot; #alias__, \</span><br><span class="line">        __FILE__);                                                            \</span><br><span class="line">    return 0;                                                                 \</span><br><span class="line">  &#125;                                                                           \</span><br><span class="line">  ParamTypeRegistry(                                                          \</span><br><span class="line">      op_type__, target__, precision__, layout__, KernelClass, alias__)</span><br></pre></td></tr></table></figure>
<h1 id="program"><a href="#program" class="headerlink" title="program"></a>program</h1><h2 id="scope"><a href="#scope" class="headerlink" title="scope"></a>scope</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Scope final &#123;</span><br><span class="line"> public:</span><br><span class="line">  Scope()</span><br><span class="line">      : kids_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class="line">        vars_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class="line">        rwlock_&#123;new lite::fluid::RWLock&#125; &#123;&#125;</span><br><span class="line">  &#x2F;&#x2F; delete below two functions to allow pybind to recognise it cannot make a</span><br><span class="line">  &#x2F;&#x2F; copy</span><br><span class="line">  &#x2F;&#x2F; link:</span><br><span class="line">  &#x2F;&#x2F; https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;53807248&#x2F;pybind11-returning-a-pointer-to-a-container-of-unique-ptr</span><br><span class="line">  Scope(const Scope&amp;) &#x3D; delete;</span><br><span class="line">  Scope&amp; operator&#x3D;(const Scope&amp;) &#x3D; delete;</span><br><span class="line">  ~Scope();</span><br><span class="line"></span><br><span class="line">  Scope&amp; NewScope() const;</span><br><span class="line"></span><br><span class="line">  Variable* Var(const std::string&amp; name);</span><br><span class="line"></span><br><span class="line">  Variable* LocalVar(const std::string&amp; name);</span><br><span class="line"></span><br><span class="line">  Variable* FindVar(const std::string&amp; name) const;</span><br><span class="line"></span><br><span class="line">  Variable* FindLocalVar(const std::string&amp; name) const;</span><br><span class="line"></span><br><span class="line">  const Scope* parent() const &#123; return parent_; &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Get attribute params stored in parent scopes.</span><br><span class="line">  std::vector&lt;std::string&gt; AttributeVarNames() const;</span><br><span class="line">  &#x2F;&#x2F; Following the legacy scope interface.</span><br><span class="line">  std::vector&lt;std::string&gt; LocalVarNames() const;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F;&#x2F; ------------------------------------- helper functions for Tensor</span><br><span class="line">  &#x2F;&#x2F;&#x2F; ----------------------------------</span><br><span class="line">  &#x2F;&#x2F; Create a Tensor variable. This will create a new Variable called &#96;name&#96;.</span><br><span class="line">  Tensor* NewTensor(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; Var(name);</span><br><span class="line">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  const Tensor* FindTensor(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; FindVar(name);</span><br><span class="line">    if (!var) return nullptr;</span><br><span class="line">    return &amp;var-&gt;Get&lt;Tensor&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Tensor* FindMutableTensor(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; FindVar(name);</span><br><span class="line">    if (!var) return nullptr;</span><br><span class="line">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;Tensor&gt;* NewTensorList(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; Var(name);</span><br><span class="line">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  const std::vector&lt;Tensor&gt;* FindTensorList(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; FindVar(name);</span><br><span class="line">    if (!var) return nullptr;</span><br><span class="line">    return &amp;var-&gt;Get&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;Tensor&gt;* FindMutableTensorList(const std::string&amp; name) &#123;</span><br><span class="line">    auto* var &#x3D; FindVar(name);</span><br><span class="line">    if (!var) return nullptr;</span><br><span class="line">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  &#x2F;&#x2F; Scope in &#96;kids_&#96; are owned by this class.</span><br><span class="line">  mutable std::list&lt;Scope*&gt; kids_;</span><br><span class="line">  const Scope* parent_&#123;nullptr&#125;;</span><br><span class="line">  std::map&lt;std::string, std::unique_ptr&lt;Variable&gt;&gt; vars_;</span><br><span class="line">  std::unique_ptr&lt;lite::fluid::RWLock&gt; kids_lock_&#123;nullptr&#125;;</span><br><span class="line">  std::unique_ptr&lt;lite::fluid::RWLock&gt; vars_lock_&#123;nullptr&#125;;</span><br><span class="line">  std::unique_ptr&lt;lite::fluid::RWLock&gt; rwlock_&#123;nullptr&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="optimize"><a href="#optimize" class="headerlink" title="optimize"></a>optimize</h1><h2 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * lite::Optimizer optimize a program. It utilize the mir passes to analysis the</span><br><span class="line"> * program and export an optimized program.</span><br><span class="line"> * Example :</span><br><span class="line"> *       &#x2F;&#x2F; (1) Create an optimizer</span><br><span class="line"> *       Optimizer optim(valid_places, kernel_pick_factor);</span><br><span class="line"> *       &#x2F;&#x2F; (2) add an optimizer method</span><br><span class="line"> *       optim.AddPass(&quot;post_quant_dynamic_pass&quot;);</span><br><span class="line"> *       &#x2F;&#x2F; (3) analysis a program to export an optimized program</span><br><span class="line"> *       auto program_ &#x3D; optim.Run(std::move(program));</span><br><span class="line"> *&#x2F;</span><br><span class="line">class Optimizer &#123;</span><br><span class="line"> public:</span><br><span class="line">  Optimizer(const std::vector&lt;Place&gt;&amp; valid_places,</span><br><span class="line">            core::KernelPickFactor kernel_pick_factor)</span><br><span class="line">      : valid_places_(valid_places), kernel_pick_factor_(kernel_pick_factor) &#123;</span><br><span class="line">    CHECK(!valid_places.empty()) &lt;&lt; &quot;At least one valid_place should be set&quot;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Append a pass to the optimizer.</span><br><span class="line">  void AddPass(const std::string&amp; pass_name);</span><br><span class="line">  &#x2F;&#x2F; Optimize a program to generate a runtime program.</span><br><span class="line">  std::unique_ptr&lt;RuntimeProgram&gt; Run(Program&amp;&amp; program);</span><br><span class="line"></span><br><span class="line"> protected:</span><br><span class="line">  &#x2F;&#x2F; Run all the added passes.</span><br><span class="line">  void ApplyPasses(std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphes);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Generate the optimized runtime program.</span><br><span class="line">  std::unique_ptr&lt;RuntimeProgram&gt; GenRuntimeProgram(</span><br><span class="line">      std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphs);</span><br><span class="line"></span><br><span class="line">  void InitTargetTypeTransformPass();</span><br><span class="line">  void InitControlFlowOpUnusedInputsAndOutputsEliminatePass();</span><br><span class="line">  void InitControlFlowOpSharedInputsAndOutputsPlaceSyncPass();</span><br><span class="line">  void SpecifyKernelPickTactic(core::KernelPickFactor factor);</span><br><span class="line">  Scope* exec_scope() &#123; return exec_scope_; &#125;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  std::vector&lt;Place&gt; valid_places_;</span><br><span class="line">  Scope* exec_scope_&#123;&#125;;</span><br><span class="line">  std::vector&lt;mir::Pass*&gt; passes_;</span><br><span class="line">  std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt; graphs_;</span><br><span class="line">  core::KernelPickFactor kernel_pick_factor_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>std::async、std::future</title>
    <url>/content/2021/04/05/article/std-async/</url>
    <content><![CDATA[<h2 id="std-async-用法"><a href="#std-async-用法" class="headerlink" title="std::async 用法"></a>std::async 用法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">template&lt;class Fn, class... Args&gt;</span><br><span class="line">future&lt;typename result_of&lt;Fn(Args...)&gt;::type&gt; async(launch policy, Fn&amp;&amp; fn, Args&amp;&amp;...args);</span><br></pre></td></tr></table></figure>
<ul>
<li>std::launch::async<br>系统默认，调用时创建新线程, </li>
<li>std::launch::deferred<br>延迟到std::future调用wait()或者get()时才执行，主线程调用，不创建新线程</li>
</ul>
<p>std::async 封装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">template &lt;typename F, typename... Args&gt;</span><br><span class="line">auto really_async(F&amp;&amp; f, Args&amp;&amp;... args)</span><br><span class="line">-&gt; std::future&lt;typename std::result_of&lt;F(Args...)&gt;::type&gt;</span><br><span class="line">&#123;</span><br><span class="line">    using RetType &#x3D; typename std::result_of&lt;F(Args...)&gt;::type;</span><br><span class="line">    auto func &#x3D; std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...);</span><br><span class="line">    std::packaged_task&lt;RetType()&gt; task(std::move(func));</span><br><span class="line">    auto fut &#x3D; task.get_future();</span><br><span class="line">    std::thread trd(std::move(task));</span><br><span class="line">    trd.detach();</span><br><span class="line">    return fut;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="std-future-用法"><a href="#std-future-用法" class="headerlink" title="std::future 用法"></a>std::future 用法</h2><h3 id="std-future-status-三种状态"><a href="#std-future-status-三种状态" class="headerlink" title="std::future_status 三种状态"></a>std::future_status 三种状态</h3><ul>
<li>deferred<br>异步操作待开始</li>
<li>ready<br>异步操作完成</li>
<li>timeout<br>异步操作超时</li>
</ul>
<h2 id="std-promise"><a href="#std-promise" class="headerlink" title="std::promise"></a>std::promise</h2><h2 id="std-packaged-task"><a href="#std-packaged-task" class="headerlink" title="std::packaged_task"></a>std::packaged_task</h2>]]></content>
  </entry>
  <entry>
    <title>TensorRT</title>
    <url>/content/2022/01/06/article/tensorrt/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><p><img src="https://user-images.githubusercontent.com/1312389/150677468-932f4721-78ee-4789-936d-2e0dd4c13f4c.png" alt="TensorRT"></p>
<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><p>可以使用三种方式进行安装，包括</p>
<ul>
<li>container 形式进行安装，下载<a href="http://ngc.nvidia.com/">NGC container</a>; </li>
<li>debian 形式安装</li>
<li>pip 形式进行安装</li>
</ul>
<h2 id="container-形式安装"><a href="#container-形式安装" class="headerlink" title="container 形式安装"></a>container 形式安装</h2><p>下载<a href="https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile">https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile</a><br><code>docker build -f ubuntu-18.04.Dockerfile --build-arg CUDA_VERSION=11.4.3 --tag=tensorrt-ubuntu .</code></p>
<h2 id="debian-形式安装"><a href="#debian-形式安装" class="headerlink" title="debian 形式安装"></a>debian 形式安装</h2><h2 id="pip形式进行安装"><a href="#pip形式进行安装" class="headerlink" title="pip形式进行安装"></a>pip形式进行安装</h2><p>与TensorRT包里面wheel包安装形式不同，这种方式是自己管理TensorRT安装，不需要提前安装TensorRT包。目前只支持Python 3.6～3.9和CUDA 11.4。</p>
<p>安装前的准备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m pip install nvidia-pyindex</span><br></pre></td></tr></table></figure>
<p>pip install时需要额外指定<code>--extra-index-url https://pypi.ngc.nvidia.com</code></p>
<p>安装TensorRT wheel包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m pip install --upgrade nvidia-tensorrt</span><br></pre></td></tr></table></figure>
<p>进行验证</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3</span><br><span class="line">&gt;&gt;&gt; import tensorrt</span><br><span class="line">&gt;&gt;&gt; print(tensorrt.__version__)</span><br><span class="line">&gt;&gt;&gt; assert tensorrt.Builder(tensorrt.Logger())</span><br></pre></td></tr></table></figure>

<h1 id="TensorRT生态"><a href="#TensorRT生态" class="headerlink" title="TensorRT生态"></a>TensorRT生态</h1><h2 id="basic-workflow"><a href="#basic-workflow" class="headerlink" title="basic workflow"></a>basic workflow</h2><p><img src="https://user-images.githubusercontent.com/1312389/150677789-25ed7568-b01d-4ccc-9e15-0266a61ae2c2.png" alt="workflow"></p>
<h2 id="convert"><a href="#convert" class="headerlink" title="convert"></a>convert</h2><ul>
<li>使用<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt">TF-TRT</a></li>
<li>使用<a href="https://github.com/NVIDIA/Torch-TensorRT">Torch-TensorRT</a></li>
<li>onnx转换器转换.onnx模型</li>
<li>使用<a href="https://docs.nvidia.com/deeplearning/tensorrt/api/index.html">TensorRT API</a>进行组网</li>
</ul>
<h2 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h2><ul>
<li><p>使用 TensorFlow</p>
<p>使用 TensorFflow 模型部署即可，TensorRT不支持的OP，会fall back到TensorFlow实现。</p>
</li>
<li><p>使用 TRT Runtime API</p>
<p>开销最小，能实现细粒度控制。对于不是原生支持的OP，需要使用plugin进行实现</p>
</li>
<li><p>使用 <a href="https://github.com/triton-inference-server/server">Nvidia Triton Inference Server</a></p>
<p>能支持多种框架，包括 TensorFlow, TensorRT, PyTorch, ONNX Runtime, 或者自定义框架。</p>
</li>
</ul>
<h1 id="TensorRT-基础介绍"><a href="#TensorRT-基础介绍" class="headerlink" title="TensorRT 基础介绍"></a>TensorRT 基础介绍</h1><h2 id="创建引擎"><a href="#创建引擎" class="headerlink" title="创建引擎"></a>创建引擎</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Logger gLogger;</span><br><span class="line">IBuilder* builder &#x3D; createInferBuilder(gLogger);</span><br><span class="line">nvinfer1::INetworkDefinition* network &#x3D; builder-&gt;createNetworkV2(1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure>
<h2 id="构建推理"><a href="#构建推理" class="headerlink" title="构建推理"></a>构建推理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IBuilderConfig* config &#x3D; builder-&gt;createBuilderConfig();</span><br><span class="line">config-&gt;setMemoryPoolLimit(1 &lt;&lt; 20);</span><br><span class="line">&#x2F;&#x2F;设置推理精度</span><br><span class="line">config-&gt;setFlag(nvinfer1::BuilderFlag::kFP16);</span><br><span class="line"></span><br><span class="line">engine &#x3D; builder-&gt;buildSerializedNetwork(*network, *config);</span><br><span class="line">context &#x3D; engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void* buffers[n];</span><br><span class="line">engine-&gt;getBindingIndex(</span><br><span class="line">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br></pre></td></tr></table></figure>
<h2 id="dynamic-shape"><a href="#dynamic-shape" class="headerlink" title="dynamic shape"></a>dynamic shape</h2><p> createNetwork()与createNetworkV2()的区别有两处，一是前者处理的维度为(C,H,W), 后者为(B,C,H,W)；二是后者支持dynamic shapes。</p>
<h2 id="plugin"><a href="#plugin" class="headerlink" title="plugin"></a>plugin</h2><ul>
<li>createNetwork()</li>
<li>createNetworkV2()<h2 id="TensorRT-优化"><a href="#TensorRT-优化" class="headerlink" title="TensorRT 优化"></a>TensorRT 优化</h2><a href="https://blog.csdn.net/qq_33287871/article/details/117201271">https://blog.csdn.net/qq_33287871/article/details/117201271</a></li>
<li>Weight &amp;Activation Precision Calibration</li>
<li>Layer &amp; Tensor Fusion</li>
<li>Kernel Auto-Tuning</li>
<li>Dynamic Tensor Memory</li>
<li>Multi-Stream Execution</li>
</ul>
<h1 id="TensorRT-API"><a href="#TensorRT-API" class="headerlink" title="TensorRT API"></a>TensorRT API</h1> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bool reshapeWeights(</span><br><span class="line">    const Weights&amp; input, int32_t const* shape, int32_t const* shapeOrder, void* data, int32_t nbDims) noexcept;</span><br><span class="line">bool reorderSubBuffers(</span><br><span class="line">    void* input, int32_t const* order, int32_t num, int32_t size) noexcept;</span><br><span class="line">bool transposeSubBuffers(</span><br><span class="line">    void* input, DataType type, int32_t num, int32_t height, int32_t width) noexcept;</span><br></pre></td></tr></table></figure>
<h1 id="TensorRT-常见问题"><a href="#TensorRT-常见问题" class="headerlink" title="TensorRT 常见问题"></a>TensorRT 常见问题</h1><h1 id="DLA"><a href="#DLA" class="headerlink" title="DLA"></a>DLA</h1><h2 id="DLA-Supported-Layers"><a href="#DLA-Supported-Layers" class="headerlink" title="DLA Supported Layers"></a>DLA Supported Layers</h2><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p><a href="https://github.com/NVIDIA/TensorRT">TensorRT</a></p>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/">TensorRT Developer Guide</a></p>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/api/index.html">TensorRT API</a></p>
<p><a href="https://github.com/onnx/onnx-tensorrt">ONNX-TensorRT</a></p>
<p><a href="https://github.com/NVIDIA/Torch-TensorRT">Torch-TensorRT</a></p>
]]></content>
      <tags>
        <tag>GPU, TensorRT</tag>
      </tags>
  </entry>
  <entry>
    <title>常用命令</title>
    <url>/content/2021/03/11/article/shell/</url>
    <content><![CDATA[<p>记录linux常用命令</p>
<h1 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h1><ul>
<li>目录下文件中指定字符串替换<br>将当前目录包括子目录的文件中<code>printf</code>字符串替换为<code>print</code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &quot;s&#x2F;printf&#x2F;print&#x2F;g&quot; &#96;grep printf -rl .&#x2F;&#96;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>线程池</title>
    <url>/content/2022/04/05/article/thread-pool/</url>
    <content><![CDATA[<h1 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h1><p>支持单例使用，支持任意参数的任务提交</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#pragma once</span><br><span class="line"></span><br><span class="line">#include &lt;condition_variable&gt;</span><br><span class="line">#include &lt;functional&gt;</span><br><span class="line">#include &lt;future&gt;</span><br><span class="line">#include &lt;memory&gt;</span><br><span class="line">#include &lt;mutex&gt;</span><br><span class="line">#include &lt;queue&gt;</span><br><span class="line">#include &lt;thread&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line"></span><br><span class="line">class ThreadPool &#123;</span><br><span class="line"> public:</span><br><span class="line">  using Task &#x3D; std::function&lt;void()&gt;;</span><br><span class="line">  explicit ThreadPool(int num_threads): running_(true) &#123;</span><br><span class="line">    threads_.resize(num_threads);</span><br><span class="line">    for (auto&amp; thread : threads_) &#123;</span><br><span class="line">      thread.reset(new std::thread(&amp;ThreadPool::TaskLoop, this));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ~ThreadPool() &#123;</span><br><span class="line">    &#123;</span><br><span class="line">      std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class="line">      running_ &#x3D; false;</span><br><span class="line">    &#125;</span><br><span class="line">    scheduled_.notify_all();</span><br><span class="line">    for (auto&amp; thread : threads_) &#123;</span><br><span class="line">      thread-&gt;join();</span><br><span class="line">      thread.reset(nullptr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  static ThreadPool* GetInstance() &#123;</span><br><span class="line">    std::call_once(init_flag_, &amp;ThreadPool::Init);</span><br><span class="line">    return threadpool_.get();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  ThreadPool(const ThreadPool&amp; pool) &#x3D; delete;</span><br><span class="line">  ThreadPool&amp; operator&#x3D;(const ThreadPool&amp; pool) &#x3D; delete;</span><br><span class="line">  template&lt;class F, class... Args&gt;</span><br><span class="line">  auto Commit(F&amp;&amp; f, Args&amp;&amp;... args) -&gt; std::future&lt;decltype(f(args...))&gt; &#123;</span><br><span class="line">    if (!running_) &#123;</span><br><span class="line">      throw std::runtime_error(&quot;ThreadPool is not running&quot;);  </span><br><span class="line">    &#125;</span><br><span class="line">    using RetType &#x3D; decltype(f(args...));</span><br><span class="line">    auto task &#x3D; std::make_shared&lt;std::packaged_task&lt;RetType()&gt;&gt;(</span><br><span class="line">      std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...)</span><br><span class="line">    );</span><br><span class="line">    std::future&lt;RetType&gt; future &#x3D; task -&gt; get_future();</span><br><span class="line">    &#123;</span><br><span class="line">        std::lock_guard&lt;std::mutex&gt; lock(mutex_);</span><br><span class="line">        tasks_.emplace([task]() &#123;</span><br><span class="line">            (*task)();</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    scheduled_.notify_one();</span><br><span class="line">    return std::move(future);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  void TaskLoop() &#123;</span><br><span class="line">    while (true) &#123;</span><br><span class="line">      Task task;</span><br><span class="line">      &#123;</span><br><span class="line">        std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class="line">        scheduled_.wait(</span><br><span class="line">            lock, [this] &#123; return !this-&gt;tasks_.empty() || !this-&gt;running_; &#125;);</span><br><span class="line">        if (!running_ &amp;&amp; tasks_.empty()) &#123;</span><br><span class="line">          return;</span><br><span class="line">        &#125;</span><br><span class="line">        task &#x3D; std::move(tasks_.front());</span><br><span class="line">        tasks_.pop();</span><br><span class="line">      &#125;</span><br><span class="line">      task();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  static void Init() &#123;</span><br><span class="line">    if (threadpool_ &#x3D;&#x3D; nullptr) &#123;</span><br><span class="line">      int num_threads &#x3D; std::thread::hardware_concurrency();</span><br><span class="line">      threadpool_.reset(new ThreadPool(num_threads));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> private:</span><br><span class="line">  static std::unique_ptr&lt;ThreadPool&gt; threadpool_;</span><br><span class="line">  static std::once_flag init_flag_;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;std::unique_ptr&lt;std::thread&gt;&gt; threads_;</span><br><span class="line">  std::queue&lt;Task&gt; tasks_;</span><br><span class="line">  std::mutex mutex_;</span><br><span class="line">  bool running_;</span><br><span class="line">  std::condition_variable scheduled_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">std::unique_ptr&lt;ThreadPool&gt; ThreadPool::threadpool_ &#x3D; nullptr;</span><br><span class="line">std::once_flag ThreadPool::init_flag_;</span><br></pre></td></tr></table></figure>

<h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">#include &quot;thread_pool.h&quot;</span><br><span class="line"></span><br><span class="line">struct sum &#123;</span><br><span class="line">  int operator()(int a, int b) &#123;</span><br><span class="line">      int res &#x3D; a + b;</span><br><span class="line">      return res;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int print(int a) &#123;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class A &#123;</span><br><span class="line">public:</span><br><span class="line">  static int calc(int val) &#123;</span><br><span class="line">      return val;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">int main() &#123;</span><br><span class="line">  ThreadPool executor(10);</span><br><span class="line">  auto result &#x3D; executor.Commit(&amp;print, 3);</span><br><span class="line">  std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; auto result &#x3D; executor.Commit(A::calc, 3);</span><br><span class="line">  &#x2F;&#x2F; std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class="line">  ThreadPool pool(4);</span><br><span class="line">  std::vector&lt;std::future&lt;int&gt;&gt; results;</span><br><span class="line">  std::chrono::seconds span(1);</span><br><span class="line">  for (int i &#x3D; 0; i &lt; 2; ++ i) &#123;</span><br><span class="line">    results.emplace_back(</span><br><span class="line">        pool.Commit([i, span] &#123;</span><br><span class="line">            std::cout &lt;&lt; &quot;run &quot; &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">            std::this_thread::sleep_for(span);</span><br><span class="line">            return i * i;</span><br><span class="line">        &#125;)</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for (auto &amp;&amp; item : results) &#123;</span><br><span class="line">      if(item.wait_for(span) &#x3D;&#x3D; std::future_status::ready) &#123;</span><br><span class="line">        std::cout &lt;&lt; item.get() &lt;&lt; std::endl;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="核心要点"><a href="#核心要点" class="headerlink" title="核心要点"></a>核心要点</h1><ul>
<li>工作队列 work queu</li>
<li>thread factory</li>
<li>饱和策略 handler</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>macOS tips</title>
    <url>/content/2021/12/09/article/use-macos/</url>
    <content><![CDATA[<h1 id="macOS-python-load-Framework"><a href="#macOS-python-load-Framework" class="headerlink" title="macOS python load Framework"></a>macOS python load Framework</h1><p><a href="https://docs.python.org/3/library/ctypes.html">https://docs.python.org/3/library/ctypes.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ctypes.util import find_library</span><br><span class="line">from ctypes import cdll</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">print(os.name)</span><br><span class="line">metal_library &#x3D; find_library(&quot;Metal&quot;)</span><br><span class="line">core_graphics_library &#x3D; find_library(&quot;CoreGraphics&quot;)</span><br><span class="line">mps_library &#x3D; find_library(&quot;MetalPerformanceShaders&quot;)</span><br><span class="line"></span><br><span class="line">print(cdll.LoadLibrary(metal_library))</span><br><span class="line">print(cdll.LoadLibrary(core_graphics_library))</span><br><span class="line">print(cdll.LoadLibrary(mps_library))</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>可变模版参数</title>
    <url>/content/2021/04/04/article/variadic-templates/</url>
    <content><![CDATA[<h1 id="c-11-可变模版参数"><a href="#c-11-可变模版参数" class="headerlink" title="c++11 可变模版参数"></a>c++11 可变模版参数</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">template &lt;class... T&gt;</span><br><span class="line">void f(T... args);</span><br></pre></td></tr></table></figure>
<ul>
<li>递归函数展开参数包<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">template &lt;class... T&gt;</span><br><span class="line">void f(T... args)</span><br><span class="line">&#123;    </span><br><span class="line">  std::cout &lt;&lt; sizeof...(args) &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void func() &#123;&#125;</span><br><span class="line">template &lt;class T, class... Args&gt;</span><br><span class="line">void func(T first, Args... remain) &#123;</span><br><span class="line">  std::cout &lt;&lt; first &lt;&lt; &quot; &quot;;</span><br><span class="line">  if (sizeof...(remain) &#x3D;&#x3D; 0) return;</span><br><span class="line">  func(remain...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">  func(2, 3, 9);</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>逗号表达式展开参数包</li>
</ul>
]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>VPN service</title>
    <url>/content/2023/01/02/article/vpn-service/</url>
    <content><![CDATA[<h1 id="使用VPS搭建VPN代理"><a href="#使用VPS搭建VPN代理" class="headerlink" title="使用VPS搭建VPN代理"></a>使用VPS搭建VPN代理</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><img width="572" alt="image" src="https://user-images.githubusercontent.com/1312389/210215276-f2394f9e-32b3-47ab-8925-0dcc104adadb.png">

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="1、免费域名"><a href="#1、免费域名" class="headerlink" title="1、免费域名"></a>1、免费域名</h3><ul>
<li>freenom免费域名申请<br> 申请地址：<a href="https://my.freenom.com/">https://my.freenom.com/</a><br> 申请时需要保证个人资料地址信息与网络ip地址信息一致；国内ip环境，需使用Gooreplacer chrome插件将<a href="http://www.google.com/recaptcha">www.google.com/recaptcha</a> 重定向recaptcha.net/recaptcha</li>
<li>eu.org免费域名申请<br>申请地址：<a href="https://nic.eu.org/arf/en">https://nic.eu.org/arf/en</a> (需使用代理)</li>
<li><a href="https://pp.ua/">https://pp.ua/</a> 免费域名申请</li>
</ul>
<h3 id="2、域名解析"><a href="#2、域名解析" class="headerlink" title="2、域名解析"></a>2、域名解析</h3><ul>
<li><a href="https://topdn.net/">https://topdn.net</a><br> 配置简单，更新快速</li>
<li>cloudflare<br>个人推荐cloudflare，功能齐全，同时能实现ip地址隐藏</li>
</ul>
<h3 id="3、CDN（可选）"><a href="#3、CDN（可选）" class="headerlink" title="3、CDN（可选）"></a>3、CDN（可选）</h3><p>   <a href="https://www.cloudflare.com/">https://www.cloudflare.com/</a> 可以实现vps ip地址隐藏，同时也可以解析到境外已被墙ip（例如阿里云香港主机）</p>
<h2 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h2><h3 id="1、v2ray或者trojan服务器伪装"><a href="#1、v2ray或者trojan服务器伪装" class="headerlink" title="1、v2ray或者trojan服务器伪装"></a>1、v2ray或者trojan服务器伪装</h3><h3 id="2、客户端v2rayNG配置"><a href="#2、客户端v2rayNG配置" class="headerlink" title="2、客户端v2rayNG配置"></a>2、客户端v2rayNG配置</h3><h3 id="3、CDN流量中转（可选）"><a href="#3、CDN流量中转（可选）" class="headerlink" title="3、CDN流量中转（可选）"></a>3、CDN流量中转（可选）</h3><p>   流量中转目的：1、隐藏VPS ip；2、解救被海外封ip</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://v2xtls.org/v2ray%E4%BD%BF%E7%94%A8cloudflare%E4%B8%AD%E8%BD%AC%E6%B5%81%E9%87%8F%EF%BC%8C%E6%8B%AF%E6%95%91%E8%A2%AB%E5%A2%99ip/">v2ray使用cloudflare中转流量，拯救被墙ip</a></li>
<li><a href="https://itlanyan.com/v2ray-traffic-mask/">V2Ray高级技巧：流量伪装</a></li>
<li><a href="https://itlanyan.com/recovery-blocked-ip/">拯救被墙的服务器</a></li>
<li><a href="https://itlanyan.com/introduce-v2ray-vless-protocol/">V2ray的VLESS协议介绍和使用教程</a></li>
<li><a href="https://itlanyan.com/trojan-tutorial/">trojan教程</a></li>
<li><a href="https://www.vjsun.com/656.html">未来的霸主选项：Xray（XTLS+V2ray)</a></li>
<li><a href="https://ssrvps.org/archives/7772">Trojan-Go一键安装脚本（Debian/Ubuntu） Trojan-Go搭建/支持Cloudflare CDN</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>work stealing</title>
    <url>/content/2022/03/19/article/workstealing/</url>
    <content><![CDATA[<h1 id="work-stealing"><a href="#work-stealing" class="headerlink" title="work stealing"></a>work stealing</h1>]]></content>
  </entry>
  <entry>
    <title>gcc</title>
    <url>/content/2021/04/14/article/cplusplus/gcc/</url>
    <content><![CDATA[<h1 id="content"><a href="#content" class="headerlink" title="content"></a>content</h1><h2 id="RVO-Return-Value-Optimization"><a href="#RVO-Return-Value-Optimization" class="headerlink" title="RVO(Return Value Optimization)"></a>RVO(Return Value Optimization)</h2><h2 id="NRVO-Named-Return-Value-Optimization"><a href="#NRVO-Named-Return-Value-Optimization" class="headerlink" title="NRVO(Named Return Value Optimization)"></a>NRVO(Named Return Value Optimization)</h2><h2 id="code-snipts"><a href="#code-snipts" class="headerlink" title="code snipts"></a>code snipts</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;algorithm&gt;</span><br><span class="line">#include &lt;cctype&gt;</span><br><span class="line"></span><br><span class="line">std::string lower(const std::string &amp;data) &#123;</span><br><span class="line">  std::string result &#x3D; data;</span><br><span class="line">  std::transform(result.begin(), result.end(), result.begin(),</span><br><span class="line">    [](unsigned char c)&#123; return std::tolower(c); &#125;);</span><br><span class="line">  return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
</search>
